{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Введение в обучение с подкреплением  (Reinforcement Learning)\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/jcformanek/jcformanek.github.io/master/docs/assets/images/rl_in_space.png\" width=\"100%\" />\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "**Введение**\n",
        "\n",
        "На этом семинаре мы изучим обучение с подкреплением - тип машинного обучения без учителя, где агент учится выбирать действия, которые приводят к максимальному вознаграждению в долгосрочной перспективе. Обучение с подкреплением используется в широком спектре сложных задач, в том числе - в видеоиграх, например, [Atari](https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark), [StarCraft II](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii) и [Dota II](https://openai.com/five/).\n",
        "\n",
        "На семинаре изучим классическую среду - [CartPole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/), где агент должен научиться балансировать шестом на тележке (см лекцию), используя несколько различных подходов RL. По пути вы познакомитесь с некоторыми из самых важных концепций и терминологии в RL.\n",
        "\n",
        "**Темы:**\n",
        "* Reinforcement Learning (RL, Обучение с подкреплением)\n",
        "* Random Policy Search (Случайный поиск политики)\n",
        "* Policy Gradient (Градиент политики)\n",
        "* Q-Learning\n",
        "\n",
        "\n",
        "**Что надо сделать для лучшего понимания:**\n",
        "\n",
        "* почитать про библиотеку [JAX](https://github.com/google/jax).\n",
        "* вспомнить нейроки.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "### Настраиваем среду"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gwbqggmcRjMy"
      },
      "outputs": [],
      "source": [
        "# @title Импорт необходимых пакетов (просто запустите ячейку) { display-mode: \"form\" }\n",
        "import copy\n",
        "from shutil import rmtree # deleting directories\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "from gym.wrappers import RecordVideo\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import matplotlib.pyplot as plt # graph plotting library\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import chex\n",
        "\n",
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## 1. Ключевые концепции обучения с подкреплением\n",
        "\n",
        "Обучение с подкреплением (RL) является подразделом машинного обучения (ML). В отличие от таких областей, как контролируемое обучение, где мы даем примеры ожидаемого поведения нашим моделям, RL фокусируется на *целеориентированном* обучении из взаимодействий методом проб и ошибок. Алгоритмы RL изучают, что делать (т. е. какие оптимальные действия предпринять) в среде, чтобы максимизировать некий сигнал вознаграждения. В таких условиях, как видеоигра, сигналом вознаграждения может быть счет игры, т. е. алгоритмы RL будут пытаться максимизировать счет в игре, выбирая лучшие действия.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pettingzoo.farama.org/_images/environments-demo.gif\" width=\"40%\" />\n",
        "</center>\n",
        "\n",
        "[*Image Source*](https://pettingzoo.farama.org/)\n",
        "\n",
        "Другими словами, в RL у нас есть **агент**, который воспринимает **наблюдение** $o_t$ текущего состояния $s_t$ **окружающей среды** и должен выбрать **действие** $a_t$ для выполнения.\n",
        "\n",
        "Затем окружающая среда переходит в новое состояние $s_{t+1}$ в ответ на действие агента, а также дает агенту скалярное вознаграждение $r_t$, чтобы указать, насколько хорошим или плохим было выбранное действие, учитывая состояние окружающей среды.\n",
        "\n",
        "Целью в RL является максимизация агентом суммы вознаграждения, которое он получает от окружающей среды с течением времени. Нижний индекс $t$ используется для указания номера временного шага, т. е. $s_0$ - это состояние окружающей среды на начальном временном шаге, а $a_{99}$ - это действие агента на $99-м$ временном шаге."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghgy69hFRjMz"
      },
      "source": [
        "### Среда - OpenAI Gym\n",
        "Как упоминалось выше, среда получает от агента действие $a_t$ и возвращает вознаграждение $r_t$ и наблюдение $o_t$.\n",
        "\n",
        "OpenAI разработал пакет для Python под названием **Gym**, который включает реализации популярных сред и простой интерфейс для взаимодействия агента RL. Чтобы использовать поддерживаемую [среду gym](https://www.gymlibrary.ml/), все, что вам нужно сделать, это передать имя среды в функцию  `gym.make(<environment_name>)`.\n",
        "\n",
        "В этом уроке мы будем использовать простую среду под названием **CartPole**. В CartPole задача агента — научиться балансировать на шесте как можно дольше, перемещая тележку *влево* или *вправо*.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/10854026/32486103-f15f19de-c3a5-11e7-81de-0605df939497.gif\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WfxzajMYRjMz"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env_name = \"CartPole-v0\"\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BbftaJj3zu"
      },
      "source": [
        "### Состояние (States) и Наблюдение (Observations) - $s_t$ and $o_t$\n",
        "\n",
        "В RL агент воспринимает наблюдение за состоянием среды. В некоторых настройках наблюдение может включать всю информацию, лежащую в основе состояния среды. Такая среда называется **полностью наблюдаемой**. В других настройках агент может получать только частичную информацию о состоянии среды в своем наблюдении. Такая среда называется **частично наблюдаемой**.\n",
        "\n",
        "В оставшейся части этого руководства мы будем предполагать, что среда полностью наблюдаема, и поэтому будем использовать состояние $s_t$ и наблюдение $o_t$ взаимозаменяемо. В Gym мы получаем начальное наблюдение из среды, вызывая функцию `env.reset()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdS8nqOgRjM0",
        "outputId": "5d44fef6-51a5-4dbb-f28a-9e566158cb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial State:: (array([-0.04766093,  0.02139146,  0.01215947, -0.04640079], dtype=float32), {})\n",
            "Environment Obs Space Shape: (4,)\n"
          ]
        }
      ],
      "source": [
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)\n",
        "\n",
        "# Get environment obs space\n",
        "obs_shape = env.observation_space.shape\n",
        "print(\"Environment Obs Space Shape:\", obs_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUNX6mbotABo"
      },
      "source": [
        "В CartPole состояние окружающей среды представлено четырьмя числами: *угловое положение полюса, угловая скорость полюса, положение тележки, скорость тележки*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1Nkgy7nUfn"
      },
      "source": [
        "### Действия (Actions) - $a_t$\n",
        "\n",
        "В RL действия обычно либо **дискретные**, либо **непрерывные**. Непрерывные действия задаются вектором действительных чисел. Дискретные действия задаются целочисленным значением. В средах, где мы можем подсчитать конечный набор действий, мы обычно используем дискретные действия.\n",
        "\n",
        "В CartPole есть только два действия: *влево* и *вправо*. Таким образом, действия могут быть представлены целыми числами $0$ и $1$. В gym мы можем легко получить список возможных действий следующим образом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOLZqU_LpIXh",
        "outputId": "1348b04b-3d83-49ee-f34b-053069cdd7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment action space: Discrete(2)\n",
            "Number of actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Get action space - e.g. discrete or continuous\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsflxbDpoPm"
      },
      "source": [
        "### Политика агента (The Agent's Policy) - $\\pi$\n",
        "\n",
        "В RL агент выбирает действия на основе полученных наблюдений. Мы можем представить процесс выбора действия агента как функцию, которая принимает наблюдение в качестве входных данных и возвращает действие в качестве выходных данных. В RL мы обычно называем эту функцию **политикой** агента и обозначаем ее $\\pi(s_t)=a_t$. В RL мы обычно параметризуем нашу политику каким-либо образом, а затем пытаемся узнать оптимальные параметры. Параметризованная политика обычно обозначается $\\pi_\\theta$, где $\\theta$ — это набор параметров.\n",
        "\n",
        "**Упражнение 1:** В качестве упражнения давайте реализуем простую политику, которая принимает набор параметров и наблюдение в качестве входных данных и возвращает действие. Предположим, что наблюдение — это вектор из четырех чисел, как наблюдение CartPole, и что действие должно быть либо $0$, либо $1$. Предположим также, что параметры — это вектор из четырех действительных чисел. Тогда действие должно быть вычислено следующим образом.\n",
        "\n",
        "\n",
        "1. Вычислите [скалярное произведение векторов](https://www.mathsisfun.com/algebra/vectors-dot-product.html) между наблюдением и параметрами.\n",
        "2. Если результат больше нуля, верните действие $1$.\n",
        "3. В противном случае верните действие $0$.\n",
        "\n",
        "В этом упражнении мы постараемся использовать JAX как можно чаще. Поэтому постарайтесь использовать методы JAX для этой задачи. Ниже приведены некоторые полезные методы, которые вы можете использовать. Вам нужно будет завершить код в блоке ниже, заменив `...` правильным кодом.\n",
        "\n",
        "\n",
        "**Полезные методы:**\n",
        "* Вычислите скалярное произведение векторов с помощью `jax.numpy.dot` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)).\n",
        "\n",
        "* Когда вы пытаетесь условно присвоить значение $0$ или $1$ действию на основе результата скалярного произведения, вам следует использовать `jax.lax.select` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select)).\n",
        "\n",
        "Метод `jax.lax.select` принимает три аргумента в качестве входных данных. Первый аргумент — это оператор, который будет оцениваться как `True` или `False`.\n",
        "\n",
        "Если оператор равен `True`, то `jax.lax.select` вернет свой второй аргумент.\n",
        "\n",
        "Если оператор имеет значение `False`, `jax.lax.select` вернет свой третий аргумент.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KSGCd7XB1z8k"
      },
      "outputs": [],
      "source": [
        "def linear_policy(params, obs):\n",
        "  \"\"\"A simple linear policy\n",
        "\n",
        "  Args:\n",
        "    params: a vector of four real-numbers that give the parameters of the policy\n",
        "    obs: a vector of four real-numbers that give the agent's observation\n",
        "\n",
        "  Returns:\n",
        "    a discrete action given by a 0 or 1\n",
        "  \"\"\"\n",
        "  # YOUR CODE\n",
        "  dot_product_result = ...\n",
        "\n",
        "  action = jax.lax.select(\n",
        "      ..., # boolean statement goes here\n",
        "      ..., # result when the statement is True goes here\n",
        "      ..., # result when the statement is False goes here\n",
        "  )\n",
        "  # END YOUR CODE\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnnBvceb1f3",
        "outputId": "92f91f76-1fc9-42fb-8cbe-d081abf0aeab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:2024-12-24 16:05:55,068:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: Error interpreting argument to <function select_n at 0x77e3a41ee710> as an abstract array. The problematic value is of type <class 'ellipsis'> and was passed to the function at path args[0].\n",
            "This typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.\n"
          ]
        }
      ],
      "source": [
        "# @title Проверка упр 1  { display-mode: \"form\" }\n",
        "\n",
        "def check_linear_policy(linear_policy):\n",
        "  fixed_obs = jnp.array([1,1,2,4])\n",
        "\n",
        "  # check case1 - negative dot product.\n",
        "  # weights\n",
        "  params1 = jnp.array([1,1,1,1])\n",
        "  params2 = jnp.array([-1,-1,-1,-1])\n",
        "\n",
        "  hint1 = f\"Неправильный ответ, ваша линейная политика неверна. Действие при \\\n",
        "obs={fixed_obs} и params={params1} должно быть 1\"\n",
        "\n",
        "  hint2 = f\"Неправильный ответ, ваша линейная политика неверна. Действие при \\\n",
        "obs={fixed_obs} и params={params2} должно быть 0\"\n",
        "\n",
        "  hint = None\n",
        "  if linear_policy(params1, fixed_obs) != 1:\n",
        "    hint = hint1\n",
        "  elif linear_policy(params2, fixed_obs) != 0:\n",
        "    hint = hint2\n",
        "\n",
        "  if hint is not None:\n",
        "    print(hint)\n",
        "  else:\n",
        "    print(\"Your function is correct!\")\n",
        "\n",
        "try:\n",
        "  check_linear_policy(linear_policy)\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkuvT-jf6Ieh"
      },
      "source": [
        "### Функция перехода среды - $P$\n",
        "\n",
        "Теперь, когда у нас есть политика, мы можем передавать действия от агента среде. Затем среда перейдет в новое состояние в ответ на действие агента.\n",
        "\n",
        "В RL мы моделируем этот процесс с помощью **функции перехода состояния** $P$, которая принимает текущее состояние $s_t$ и действие $a_t$ в качестве входных данных и возвращает следующее состояние $s_{t+1}$ в качестве выходных данных:\n",
        "\n",
        "<center>\n",
        "$s_{t+1}=P(s_t, a_t)$\n",
        "</center>\n",
        "\n",
        "В gym мы можем передавать действия среде, вызывая функцию `env.step(<action>)`. Затем функция вернет четыре значения:\n",
        "- **следующее наблюдение**\n",
        "- **награду** за выполненное действие\n",
        "- логический флаг, указывающий, **завершена** игра\n",
        "- некоторую **дополнительную** информацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh0j9-Tk7olb",
        "outputId": "e381247a-5ae8-4a21-ac97-583dbfcb38a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation: (array([-0.03986551,  0.00638674, -0.02944808,  0.03795552], dtype=float32), {})\n",
            "Action: 0\n",
            "Next observation: [-0.03973778 -0.18830082 -0.02868897  0.32120377]\n",
            "Reward: 1.0\n",
            "Game is done: False\n"
          ]
        }
      ],
      "source": [
        "# Get the initial obs by resetting the env\n",
        "initial_obs = env.reset()\n",
        "\n",
        "# Randomly sample actions from env\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# Step the environment\n",
        "#next_obs, reward, done, info = env.step(action) тут выдает ошибку\n",
        "next_obs, reward, done, info, terminated = env.step(action)\n",
        "\n",
        "print(\"Observation:\", initial_obs)\n",
        "print(\"Action:\", action)\n",
        "print(\"Next observation:\", next_obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game is done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9iZtu48UYn"
      },
      "source": [
        "### Возврат эпизода - $R_t$\n",
        "\n",
        "В RL мы обычно разбиваем взаимодействие агента с окружающей средой на **эпизоды**. Сумма всех вознаграждений, собранных во время эпизода, - это то, что мы называем **возвратом** эпизода - $R_t$:\n",
        "\n",
        "<center>\n",
        "$R_t=\\sum_{t=0}^Tr_t$,\n",
        "</center>\n",
        "\n",
        "где $r_t$ - это вознаграждение в момент времени $t$, а $T$ - это последний временной шаг. Цель в RL - выбрать действия, которые максимизируют этот ожидаемый будущий возврат $R_t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6KZA1Nq9p47"
      },
      "source": [
        "### Цикл агент-среда\n",
        "Теперь, когда мы знаем, что такое политика, и знаем, как пошагово проходить среду, давайте закроем цикл агент-среда.\n",
        "\n",
        "**Упражнение 2:** Напишите функцию, которая запускает один эпизод CartPole, последовательно выбирая действия и пошагово проходя среду. Вам следует использовать линейную политику, которую мы определили ранее, для выбора действий. Функция должна отслеживать полученное вознаграждение и выводить возврат в конце эпизода.\n",
        "\n",
        "В CartPole агент получает вознаграждение `1` за каждый временной шаг, когда шест все еще стоит вертикально. Если шест падает, игра заканчивается, и агент больше не получает вознаграждения. Игра также заканчивается после `200` временных шагов, поэтому максимальное вознаграждение, которое может получить агент, составляет `200`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Buy0X7mi-gHP"
      },
      "outputs": [],
      "source": [
        "def run_episode(env):\n",
        "  episode_return = 0 # counter to keep track of rewards\n",
        "  done = False # initially set to False\n",
        "  params = jnp.array([1,-2,2,-1]) # fixed policy parameters\n",
        "\n",
        "  ## YOUR CODE\n",
        "\n",
        "  obs = ... # TODO: get the initial obs from the env\n",
        "\n",
        "  while not done: # loop until episode is done\n",
        "\n",
        "    action = ... # TODO: compute action using linear policy\n",
        "    action = np.array(action) # We need to the convert the action from the policy to a np.array\n",
        "\n",
        "    obs, reward, done, info = ... # TODO: step the environment\n",
        "\n",
        "    episode_return = ... # TODO: add reward to episode return\n",
        "\n",
        "  return episode_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA2Orj9PVbKO",
        "outputId": "e144f69d-6ade-4cac-d4c0-8f2fdddc45bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: 'CartPoleEnv' object has no attribute 'seed'\n"
          ]
        }
      ],
      "source": [
        "# @title Проверка упр 2 (run me) { display-mode: \"form\" }\n",
        "\n",
        "try:\n",
        "  env.seed(42)\n",
        "  if run_episode(env) == 31:\n",
        "    print(\"Looks correct!\")\n",
        "  else:\n",
        "    print(\"Looks like your implementation might be wrong.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGdzHxJnZGl"
      },
      "source": [
        "В CartPole среда считается решенной, когда агент может надежно достичь возврата эпизода 500. Как вы можете видеть, наша текущая политика далека от оптимальной.\n",
        "\n",
        "Один из способов найти оптимальную политику — это случайным образом пробовать разные политики, пока не найдем оптимальную. Эта стратегия называется случайным поиском политики и может быть на удивление эффективной.\n",
        "\n",
        "Прежде чем мы реализуем случайный поиск политики, давайте быстро рассмотрим общий цикл обучения RL, который мы будем использовать для реализации алгоритмов в оставшейся части этого руководства.\n",
        "\n",
        "### Цикл обучения RL общего назначения\n",
        "Мы реализовали для вас цикл обучения RL общего назначения. Цикл обучения принимает несколько аргументов в качестве входных данных, но три наиболее важных для понимания — это `agent_select_action_func`, `agent_learn_func` и `agent_memory`.\n",
        "\n",
        "* `agent_select_action_func` — это функция, которую мы определяем и можем передать в цикл обучения. Функция принимает наблюдение и набор `agent_params` в качестве входных данных и должна возвращать действие.\n",
        "* `agent_learn_func` — это еще один метод, который мы определяем и передаем в цикл обучения. Он должен принимать параметры агента и некоторые «воспоминания» в качестве входных данных, а затем обновлять и возвращать агентам новые параметры.\n",
        "* `agent_memory` — это модуль общего назначения, который мы определяем, который может хранить некоторую релевантную информацию об опыте агента в среде, которую можно использовать в `agent_learn_func`.\n",
        "\n",
        "Ниже приведена функция цикла обучения, которую мы реализовали для вас. Вы можете просмотреть код и попытаться понять его, но это не обязательно. Таким образом, мы скрыли код по умолчанию, просто убедитесь, что вы запустили ячейку кода, прежде чем двигаться дальше, потому что цикл обучения далее в коде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZWBwz3zMRjM0"
      },
      "outputs": [],
      "source": [
        "#@title Training loop (run me) { display-mode: \"form\" }\n",
        "\n",
        "# NamedTuple to store transitions\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# Training Loop\n",
        "def run_training_loop(env_name, agent_params, agent_select_action_func,\n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None,\n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=100,\n",
        "    evaluation_episodes=8, learn_steps_per_episode=1,\n",
        "    train_every_timestep=False, video_subdir=\"\",):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does\n",
        "    some agent learning and evaluation.\n",
        "\n",
        "    Args:\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state\n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the\n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state\n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical\n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "        train_every_timestep: whether to train every timestep rather than at the end\n",
        "            of the episode.\n",
        "        video_subdir: subdirectory to store epsiode recordings.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup Cartpole environment and recorder\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\") # training environment\n",
        "    eval_env = gym.make(env_name, render_mode=\"rgb_array\") # evaluation environment\n",
        "\n",
        "    # Video dir\n",
        "    video_dir = \"./video\"+\"/\"+video_subdir\n",
        "\n",
        "    # Clear video dir\n",
        "    try:\n",
        "      rmtree(video_dir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # Wrap in recorder\n",
        "    env = RecordVideo(env, video_dir+\"/train\", episode_trigger=lambda x: (x % evaluator_period) == 0)\n",
        "    eval_env = RecordVideo(eval_env, video_dir+\"/eval\", episode_trigger=lambda x: (x % evaluation_episodes) == 0)\n",
        "\n",
        "    # JAX random number generator\n",
        "    rng = hk.PRNGSequence(jax.random.PRNGKey(0))\n",
        "    env.seed(0) # seed environment for reproducability\n",
        "    random.seed(0)\n",
        "\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    timesteps = 0\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng),\n",
        "                                            agent_params,\n",
        "                                            agent_actor_state,\n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "            # Increment timestep counter\n",
        "            timesteps += 1\n",
        "\n",
        "            # Maybe learn every timestep\n",
        "            if train_every_timestep and (timesteps % 4 == 0) and agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learn_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = eval_env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng),\n",
        "                                    agent_params,\n",
        "                                    agent_actor_state,\n",
        "                                    np.array(obs),\n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = eval_env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPlIq4oDBPY"
      },
      "source": [
        "##2. Случайный поиск политики (RPS)\n",
        "В разделе 1 мы использовали фиксированный набор параметров для нашей политики. То есть мы не изучали параметры $\\pi$ $\\theta$, мы просто сохранили их фиксированными ( `params = [1,-2,2,-1]`).\n",
        "\n",
        "Теперь мы реализуем случайный поиск политики (RPS), который представляет собой алгоритм, который случайным образом пробует различные параметры политики и отслеживает лучшие параметры, найденные на данный момент. Мы скажем, что параметры политики $\\theta_A$ лучше параметров $\\theta_B$, если средний возврат эпизода, достигнутый за последние 20 эпизодов политикой с параметрами $\\theta_A$, больше, чем у политики с параметрами $\\theta_B$.\n",
        "\n",
        "Чтобы отслеживать «текущие» параметры, а также «лучшие» параметры, мы будем использовать [NamedTuple](https://www.geeksforgeeks.org/namedtuple-in-python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DcaC-PQRjM1",
        "outputId": "68464fff-5a0d-4b8c-e0c1-a4c9ff730b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: [0. 0. 0. 0.]\n",
            "Current params: [-1. -1. -1. -1.]\n"
          ]
        }
      ],
      "source": [
        "# Parameter container for Random Policy Search\n",
        "RandomPolicySearchParams = collections.namedtuple(\"RandomPolicySearchParams\", [\"current\", \"best\"])\n",
        "\n",
        "# TEST: store two different sets of parameters\n",
        "current_params = np.ones(obs_shape) * -1\n",
        "best_params = np.zeros(obs_shape)\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# How to access the best or current params.\n",
        "print(f\"Best params: {rps_params.best}\")\n",
        "print(f\"Current params: {rps_params.current}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v91pDDzGex9a"
      },
      "source": [
        "Далее мы реализуем следующее:\n",
        "- **Функция выбора действия RPS** - определяет, как мы выбираем действия с учетом набора параметров.\n",
        "- **Модуль памяти RPS** - определяет, какой опыт хранить из взаимодействий со средой.\n",
        "- **Функция обучения RPS** - определяет, как мы обновляем и улучшаем параметры нашей политики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tExceGeGNYH"
      },
      "source": [
        "### Функция выбора действия RPS\n",
        "Давайте реализуем функцию с именем `random_policy_search_choose_action`, которую мы можем передать в цикл обучения. Функция должна принимать несколько аргументов, чтобы она могла хорошо взаимодействовать с нашим обобщенным циклом обучения, но вам нужно будет использовать только три из них - `params`, `obs` и `evaluation`.\n",
        "\n",
        "- `params` - это экземпляр `RandomPolicySearchParams` с \"current\" и \"best\".\n",
        "- `obs` - это последнее наблюдение из среды.\n",
        "- `evaluation` - это логическое значение, которое указывает, следует ли нам использовать параметры \"current\" или \"best\". Когда `evaluation==True`, мы должны использовать параметры \"best\", в противном случае мы должны использовать параметры \"current\".\n",
        "\n",
        "**Упражнение 3:** Реализуйте функцию `random_policy_search_choose_action`, как описано выше. Вам следует использовать метод `linear_policy`, который мы определили ранее. Вам также понадобится `jax.lax.select()` для условного возврата \"лучшего\" действия или \"текущего\" действия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1kmwT35JRjM1"
      },
      "outputs": [],
      "source": [
        "def random_policy_search_choose_action(\n",
        "    key,\n",
        "    params,\n",
        "    actor_state,\n",
        "    obs,\n",
        "    evaluation=False\n",
        "):\n",
        "  \"\"\"Random policy search select action method.\n",
        "\n",
        "  Args:\n",
        "    key: a random number (seed). Not used in this function.\n",
        "    params: the agent's parameters. In this case an instance of `RandomPolicySearchParams`\n",
        "    actor_state: some extra information about the actor. Not used in this function.\n",
        "    obs: the latest observation.\n",
        "    evaluation: a boolean indicating whether to use the best \"parameters\" or the \"current\" ones.\n",
        "\n",
        "  Returns:\n",
        "    The chosen action and the updated actor_state. In this function the actor_state is not updated.\n",
        "  \"\"\"\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  best_action = ...\n",
        "\n",
        "  current_action = ...\n",
        "\n",
        "  action = jax.lax.select(\n",
        "      ... ,\n",
        "      ... ,\n",
        "      ...\n",
        "  )\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE4xoAgyI3kG",
        "outputId": "695e05e9-268b-4b97-c81a-884fec5426fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: Error interpreting argument to <function select_n at 0x77e3a41ee710> as an abstract array. The problematic value is of type <class 'ellipsis'> and was passed to the function at path args[0].\n",
            "This typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.\n"
          ]
        }
      ],
      "source": [
        "# @title Проверка упр 3 (run me) {display-mode: \"form\"}\n",
        "\n",
        "def check_random_policy_search_choose_action(choose_action):\n",
        "  key = None # not used\n",
        "  actor_state = None # not used\n",
        "\n",
        "  # obs\n",
        "  obs = np.ones(obs_shape)\n",
        "\n",
        "  evaluation=False\n",
        "  current_params = np.ones(obs_shape) * -1\n",
        "  best_params = np.ones(obs_shape)\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  if action != 0:\n",
        "    return False\n",
        "\n",
        "  evaluation=True\n",
        "  current_params = np.ones(obs_shape) * -1\n",
        "  best_params = np.ones(obs_shape)\n",
        "  rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "  action, actor_state = choose_action(key,rps_params,actor_state,obs,evaluation)\n",
        "  if action != 1:\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "\n",
        "try:\n",
        "  if check_random_policy_search_choose_action(random_policy_search_choose_action):\n",
        "    print(\"Your function looks correct.\")\n",
        "  else:\n",
        "    print(\"Your function looks incorrect.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu51ep7Sh0M"
      },
      "source": [
        "### Память агента RPS\n",
        "\n",
        "Для алгоритма поиска случайной политики нам нужно будет отслеживать средний возврат эпизода для последних 50 эпизодов. Помните, что мы сказали, что «текущие» параметры будут считаться «лучшими» параметрами, если средний возврат эпизода при использовании этих параметров больше предыдущего лучшего среднего возврата эпизода.\n",
        "\n",
        "Мы будем использовать интерфейс памяти общего назначения, который довольно прост. Модуль памяти должен иметь три метода. Первый — это функция `memory.push(<transition>)`, которая добавляет некоторую информацию о последнем переходе среды в память. Второй — это функция `memory.is_ready()`, которая проверяет, готова ли память к обучению. Наконец, функция `memory.sample()` должна возвращать последний набор воспоминаний, которые можно передать в `agent_learn_func`.\n",
        "\n",
        "#### Память среднего возврата эпизода\n",
        "Мы создали для вас простой модуль памяти агента. Он хранит `epsisode_returns` последних 20 эпизодов. Прочитайте нашу реализацию ниже и посмотрите, сможете ли вы ее понять. Метод `memory.sample()` возвращает средний возврат эпизода за последние 20 эпизодов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YVkTBIK5RjM1"
      },
      "outputs": [],
      "source": [
        "class AverageEpisodeReturnBuffer:\n",
        "\n",
        "    def __init__(self, num_episodes_to_store=50):\n",
        "        \"\"\"\n",
        "        This class implements an agent memory that stores the average episode\n",
        "        return over the last 50 episodes.\n",
        "        \"\"\"\n",
        "        self.num_episodes_to_store = num_episodes_to_store\n",
        "        self.episode_return_buffer = []\n",
        "        self.current_episode_return = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_return += transition.reward\n",
        "\n",
        "        if transition.done: # If the episode is done\n",
        "            # Add episode return to buffer\n",
        "            self.episode_return_buffer.append(self.current_episode_return)\n",
        "\n",
        "            # Reset episode return\n",
        "            self.current_episode_return = 0\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.episode_return_buffer) == self.num_episodes_to_store\n",
        "\n",
        "    def sample(self):\n",
        "        average_episode_return = np.mean(self.episode_return_buffer)\n",
        "\n",
        "        # Clear episode return buffer\n",
        "        self.episode_return_buffer = []\n",
        "\n",
        "        return average_episode_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oXBsSa2KjWE"
      },
      "source": [
        "### Функция обучения RPS\n",
        "Наконец, нам нужно реализовать функцию `random_policy_search_learn` для нашего алгоритма поиска случайной политики. Функция обучения довольно проста. Все, что нам нужно сделать, это проверить, лучше ли текущие параметры, чем лучшие параметры. Если они лучше, то установите лучшие параметры в качестве текущих параметров и случайным образом сгенерируйте новый набор текущих параметров.\n",
        "\n",
        "**Упражнение 4:** Напишите функцию для случайной генерации новых весов с использованием JAX. Веса должны быть выбраны из интервала `[-2,2]`.\n",
        "\n",
        "**Полезные функции:**\n",
        "*   `jax.random.uniform` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html#jax.random.uniform))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8V2yFM2XMjGW"
      },
      "outputs": [],
      "source": [
        "def get_new_random_weights(random_key, old_weights, minval=-2.0, maxval=2.0):\n",
        "    new_weights_shape = old_weights.shape # you will need to use these values\n",
        "    new_weights_dtype = old_weights.dtype # you will need to use these values\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    new_params = ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return new_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8ifHTKGQlAd",
        "outputId": "c89c818c-f445-4cf2-eb97-17aa6588f6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: Value 'Ellipsis' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.\n"
          ]
        }
      ],
      "source": [
        "# @title Проверка упр 4 (run me) {display-mode: \"form\"}\n",
        "\n",
        "def check_get_new_random_weights(get_new_random_weights):\n",
        "  old_weights = np.ones(obs_shape, \"float32\")\n",
        "  random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "  # Case 1\n",
        "  new_weights = get_new_random_weights(random_key, old_weights, minval=-2.0, maxval=2.0)\n",
        "\n",
        "  if jnp.array_equal(new_weights, jnp.array([ 0.29657745,1.4265499, -1.7621555, -1.7505779 ])):\n",
        "    print(\"Function is correct!\")\n",
        "  else:\n",
        "    print(\"Something is wrong.\")\n",
        "\n",
        "try:\n",
        "  check_get_new_random_weights(get_new_random_weights)\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djxc9j-LzkM"
      },
      "source": [
        "Наша функция обучения получает память в форме среднего возврата эпизода из `AverageEpisodeReturnMemory`, которую мы реализовали ранее. Мы можем использовать это для сравнения текущих параметров с лучшими параметрами. Но нам также нужно будет отслеживать лучший средний возврат эпизода для функции обучения. Для этого мы можем использовать аргумент `learn_state`, который передается в `agent_learn_func` в нашем цикле обучения. Как и в случае с `RandomPolicySearchParams`, мы будем использовать NamedTuple для хранения `best_average_episode_return` в `learn_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cSH4uYmRjM2",
        "outputId": "8e04bd66-c373-4846-fe54-5a17bea0a8fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial best average episode return: -inf\n"
          ]
        }
      ],
      "source": [
        "# A NamedTuple to store the best average episode return so far\n",
        "RandomPolicyLearnState = collections.namedtuple(\n",
        "  \"RandomPolicyLearnState\",\n",
        "  [\"best_average_episode_return\"]\n",
        ")\n",
        "\n",
        "# Test\n",
        "initial_learn_state = RandomPolicyLearnState(best_average_episode_return=-float(\"inf\"))\n",
        "print(\"Initial best average episode return:\", initial_learn_state.best_average_episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxZXdP8bOu6b"
      },
      "source": [
        "Теперь у нас есть все необходимое для реализации функции `random_policy_search_learn`.\n",
        "\n",
        "**Упражнение 5:** Реализуйте функцию `random_policy_search_learn`. Функция должна проверять, лучше ли «текущие» параметры, чем «лучшие» параметры, сравнивая `current_average_episode_return` с `best_average_episode_return`. Функция также должна обновлять `learn_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Te_Q3qzmZcN_"
      },
      "outputs": [],
      "source": [
        "def random_policy_search_learn(key, params, learn_state, memory):\n",
        "    best_params = params.best\n",
        "    current_params = params.current\n",
        "\n",
        "    current_average_episode_return = memory # the memory contains the average episode return\n",
        "    best_average_episode_return = learn_state.best_average_episode_return\n",
        "\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    best_params = jax.lax.select(\n",
        "        ... ,\n",
        "        ... ,\n",
        "        ...\n",
        "    )\n",
        "\n",
        "    best_average_episode_return = jax.lax.select(\n",
        "        ... ,\n",
        "        ... ,\n",
        "        ...\n",
        "    )\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    # Generate new random parameters\n",
        "    new_params = get_new_random_weights(key, current_params)\n",
        "\n",
        "    # Bundle weights in RandomPolicySearchParams NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_params, best=best_params)\n",
        "\n",
        "    return params, RandomPolicyLearnState(best_average_episode_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXHV4k2iBLBF",
        "outputId": "7a88dc69-38a1-483c-af1c-56e011e7c8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: Error interpreting argument to <function select_n at 0x77e3a41ee710> as an abstract array. The problematic value is of type <class 'ellipsis'> and was passed to the function at path args[0].\n",
            "This typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 5 {display-mode: \"form\"}\n",
        "\n",
        "params = RandomPolicySearchParams(np.ones(obs_shape, \"float32\"), np.ones(obs_shape, \"float32\") * -1)\n",
        "learn_state = RandomPolicyLearnState(10)\n",
        "memory = 11\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "try:\n",
        "  new_params, new_learn_state = random_policy_search_learn(key, params, learn_state, memory)\n",
        "\n",
        "  if not jnp.array_equal(new_params.current, jnp.array([ 0.29657745,  1.4265499 , -1.7621555 , -1.7505779 ])):\n",
        "    print(\"Your function is incorrect.\")\n",
        "\n",
        "  elif not jnp.array_equal(new_params.best, jnp.array([1., 1., 1., 1.])):\n",
        "    print(\"Your function is incorrect.\")\n",
        "\n",
        "  elif new_learn_state.best_average_episode_return != 11:\n",
        "    print(\"Your function is incorrect.\")\n",
        "\n",
        "  else:\n",
        "    print(\"Your function looks correct.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ol_AxnMBdgP"
      },
      "source": [
        "### Цикл обучения RPS\n",
        "Теперь мы можем собрать все вместе, передав модуль `memory`, функцию `learn` и функцию `choose_action` в цикл обучения. Чтобы ускорить наш алгоритм, мы будем использовать `jax.jit` в функции `learn` и функции `choose_action`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Qx57tf7vRjM3",
        "outputId": "70bb7fe8-bb2d-49ea-9a4c-d3b7b0558460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training. This may take up to 5 minutes to complete.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'CartPoleEnv' object has no attribute 'seed'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training. This may take up to 5 minutes to complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m chex\u001b[38;5;241m.\u001b[39mclear_trace_counter()\n\u001b[0;32m---> 18\u001b[0m episode_return, evaluator_episode_returns \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrandom_policy_search_choose_action_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# no actor state\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrandom_policy_search_learn_jit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43minitial_learn_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mvideo_subdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Plot graph of evaluator episode returns\u001b[39;00m\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(evaluator_episode_returns)), evaluator_episode_returns)\n",
            "Cell \u001b[0;32mIn[13], line 59\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m(env_name, agent_params, agent_select_action_func, agent_actor_state, agent_learn_func, agent_learner_state, agent_memory, num_episodes, evaluator_period, evaluation_episodes, learn_steps_per_episode, train_every_timestep, video_subdir)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# JAX random number generator\u001b[39;00m\n\u001b[1;32m     58\u001b[0m rng \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mPRNGSequence(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 59\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# seed environment for reproducability\u001b[39;00m\n\u001b[1;32m     60\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m episode_returns \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# List to store history of episode returns.\u001b[39;00m\n",
            "File \u001b[0;32m~/Документы/nlpLab/nlp/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Документы/nlpLab/nlp/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[0;31m[... skipping similar frames: Wrapper.__getattr__ at line 241 (1 times)]\u001b[0m\n",
            "File \u001b[0;32m~/Документы/nlpLab/nlp/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CartPoleEnv' object has no attribute 'seed'"
          ]
        }
      ],
      "source": [
        "# JIT the learn and choose action functions\n",
        "random_policy_search_learn_jit = jax.jit(random_policy_search_learn)\n",
        "random_policy_search_choose_action_jit = jax.jit(random_policy_search_choose_action)\n",
        "\n",
        "# Initialise the parameters\n",
        "initial_weights = np.ones(obs_shape, \"float32\")\n",
        "initial_params = RandomPolicySearchParams(initial_weights, initial_weights)\n",
        "\n",
        "# Initialise the learn state\n",
        "initial_learn_state = RandomPolicyLearnState(best_average_episode_return=-float(\"inf\"))\n",
        "\n",
        "# Initialise memory\n",
        "memory = AverageEpisodeReturnBuffer(num_episodes_to_store=50)\n",
        "\n",
        "# Run the training loop\n",
        "print(\"Starting training. This may take up to 5 minutes to complete.\")\n",
        "chex.clear_trace_counter()\n",
        "episode_return, evaluator_episode_returns = run_training_loop(\n",
        "                                        env_name,\n",
        "                                        initial_params,\n",
        "                                        random_policy_search_choose_action_jit,\n",
        "                                        None, # no actor state\n",
        "                                        random_policy_search_learn_jit,\n",
        "                                        initial_learn_state,\n",
        "                                        memory,\n",
        "                                        num_episodes=1001,\n",
        "                                        video_subdir=\"rps\"\n",
        "                                    )\n",
        "\n",
        "# Plot graph of evaluator episode returns\n",
        "plt.plot(np.linspace(0, 1000, len(evaluator_episode_returns)), evaluator_episode_returns)\n",
        "plt.title(\"Random Policy Search\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG10FG6uS05A"
      },
      "source": [
        "Надеюсь, вы нашли набор оптимальных параметров на CartPole (возврат эпизода достигает `200`). В ячейке ниже вы можете посмотреть несколько видеороликов агента, выполняющего задание."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "KgayDJ7KWC4C",
        "outputId": "5302d001-826b-49ca-a885-3044299dbaae"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './video/rps/eval/rl-video-episode-80.mp4'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m eval_episode_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(episode_number \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      9\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./video/rps/eval/rl-video-episode-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_episode_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m mp4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     12\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:video/mp4;base64,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m b64encode(mp4)\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m     13\u001b[0m HTML(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m<video width=400 controls>\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m      <source src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo/mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m</video>\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m data_url)\n",
            "File \u001b[0;32m~/Документы/nlpLab/nlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './video/rps/eval/rl-video-episode-80.mp4'"
          ]
        }
      ],
      "source": [
        "#@title Visualise Policy {display-mode: \"form\"}\n",
        "#@markdown Choose an episode number that is a multiple of 100 and less than or equal to 2000, and **run** this cell.\n",
        "episode_number = 1000 #@param {type:\"number\"}\n",
        "\n",
        "assert (episode_number % 100) == 0, \"Episode number must be a multiple of 100 since we only record every 100th episode.\"\n",
        "assert episode_number < 1001, \"Episode number must be less than or equal to 2000\"\n",
        "\n",
        "eval_episode_number = int(episode_number / 100 * 8)\n",
        "video_path = f\"./video/rps/eval/rl-video-episode-{eval_episode_number}.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwf1IKKbrrVn"
      },
      "source": [
        "Итак, случайный поиск политики справился с этой задачей довольно хорошо. Однако здесь происходит очень мало (если вообще происходит) реального *обучения*. Далее давайте рассмотрим реализацию простого алгоритма RL, который может использовать свой опыт для руководства нашим поиском оптимальной политики, а не просто искать ее случайным образом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnSjZVESrxc"
      },
      "source": [
        "## 3. Градиенты политики  (Policy Gradients, PG)\n",
        "Как обсуждалось, цель RL — найти политику, которая максимизирует ожидаемое совокупное вознаграждение (возврат), которое агент получает от среды. Мы можем записать ожидаемый возврат политики как:\n",
        "\n",
        "$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)]$,\n",
        "\n",
        "где $\\pi_\\theta$ — это политика, параметризованная $\\theta$, $\\mathrm{E}$ означает *ожидание*, $\\tau$ — это сокращение для \"*эпизод*\", $\\tau\\sim\\pi_\\theta$ — это сокращение для \"*эпизоды, выбранные с использованием политики* $\\pi_\\theta$\", а $R(\\tau)$ — это возврат эпизода $\\tau$.\n",
        "\n",
        "Затем, цель в RL — найти параметры $\\theta$, которые максимизируют функцию $J(\\pi_\\theta)$. Один из способов найти эти параметры — выполнить градиентный подъем по $J(\\pi_\\theta)$ относительно параметров $\\theta$:\n",
        "\n",
        "$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}}$,\n",
        "\n",
        "где $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ — градиент ожидаемой доходности относительно параметров политики $\\theta_k$, а $\\alpha$ — размер шага. Эта величина, $\\nabla J(\\pi_\\theta)$, также называется **градиентом политики** и очень важна в RL. Если мы сможем вычислить градиент политики, то у нас будет средство, с помощью которого можно напрямую оптимизировать нашу политику.\n",
        "\n",
        "Как оказалось, у нас есть способ вычислить градиент политики, и математический вывод можно найти [здесь](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html). Но для этого руководства мы опустим вывод и просто дадим вам результат:\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)]$\n",
        "\n",
        "Неформально, градиент политики равен градиенту логарифма вероятности выбранного действия, умноженного на возврат эпизода, в котором было предпринято действие.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTnTzgtSuy-y"
      },
      "source": [
        "### REINFORCE\n",
        "REINFORCE — это простой алгоритм RL, который использует градиент политики для поиска оптимальной политики путем увеличения вероятности выбора действий (усиливающих действий), которые, как правило, приводят к эпизодам с высокой отдачей.\n",
        "\n",
        "**Упражнение 6:** Реализуйте функцию, которая берет вероятность действия и отдачу эпизода, в котором действие было предпринято, и вычисляет логарифм вероятности, умноженный на отдачу. Убедитесь, что вы используете JAX.\n",
        "\n",
        "**Полезные функции:**\n",
        "*   `jax.numpy.log`([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bJObUsoUrOyV"
      },
      "outputs": [],
      "source": [
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    log_porb = ...\n",
        "\n",
        "    weighted_log_prob = ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZPoTwa1Gbm1",
        "outputId": "141f03d9-3c4b-46a7-e294-ed4f7c1b46ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks correct.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 6 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  action_prob = 0.8\n",
        "  episode_return = 100\n",
        "  result = compute_weighted_log_prob(action_prob, episode_return)\n",
        "  if result != -22.314354:\n",
        "    print(\"Your implementation looks incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks correct.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgW9UJ3tIpl"
      },
      "source": [
        "### Rewards-to-go\n",
        "Выполнение градиентного подъема по градиенту логарифма вероятности действия, взвешенного по возврату эпизода, будет иметь тенденцию повышать вероятность действий, которые были в эпизодах с высоким возвратом, независимо от того, *где* в эпизоде ​​было выполнено действие. Это на самом деле не имеет большого смысла, потому что действие ближе к концу эпизода может быть подкреплено, потому что много вознаграждения было собрано ранее в эпизоде, *до* того, как было выполнено действие. Агенты RL должны на самом деле подкреплять действия только на основе их *последствий*. Вознаграждения, полученные до выполнения действия, не влияют на то, насколько хорошим было это действие: только вознаграждения, которые приходят после. Накопленные вознаграждения, полученные после выполнения действия, называются **вознаграждениями-к-переходу** и могут быть вычислены как:\n",
        "\n",
        "$\\hat{R}_i=\\sum_{t=i}^Tr_t$\n",
        "\n",
        "Сравните это с доходностью эпизода:\n",
        "\n",
        "$R(\\tau)=\\sum_{t=0}^Tr_t$\n",
        "\n",
        "Мы можем повысить надежность градиента политики, заменив доходность эпизода на вознаграждения-к-переходу. Градиент политики с оставшимися наградами задается следующим образом:\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t]$\n",
        "\n",
        "**Упражнение 7:** Реализуйте функцию, которая берет список всех наград, полученных в эпизоде, и вычисляет оставшиеся награды. Не беспокойтесь об использовании JAX в этой функции. Вы можете использовать обычные операции Python, такие как `for-loops`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV1Hww8E3dUJ"
      },
      "outputs": [],
      "source": [
        "def compute_rewards_to_go(rewards):\n",
        "    \"\"\"\n",
        "    This function should take a list of rewards as input and\n",
        "    compute the rewards-to-go for each timestep.\n",
        "\n",
        "    EXAMPLE: compute_rewards_to_go([1,2,3,4]) = [10, 9, 7, 4]\n",
        "\n",
        "    Arguments:\n",
        "        rewards[t] is the reward at time step t.\n",
        "\n",
        "    Returns:\n",
        "        rewards_to_go[t] should be the reward-to-go at timestep t.\n",
        "    \"\"\"\n",
        "\n",
        "    rewards_to_go = []\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return rewards_to_go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLVaVRp28YGI",
        "outputId": "a534c323-3739-4a5a-9819-032d50a216fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks correct.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 7 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = compute_rewards_to_go([1,2,3,4])\n",
        "\n",
        "  if result != [10, 9, 7, 4]:\n",
        "    print(\"There is a problem with your implementation.\")\n",
        "  else:\n",
        "    print(\"Looks correct.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboxN9MS65i5"
      },
      "source": [
        "### REINFORCE memory\n",
        "Далее нам нужно будет создать новую память агента для хранения вознаграждений $\\hat{R}_t$ вместе с наблюдением $o_t$ и действием $a_t$ на каждом временном шаге. Ниже мы реализовали такой модуль памяти для вас. Функция `memory.sample()` вернет пакет из последних 500 воспоминаний. Вы можете прочитать код, чтобы попытаться понять его, но это не обязательно. Поэтому мы скрываем код по умолчанию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhS4V6auRjM3"
      },
      "outputs": [],
      "source": [
        "# @title Реализация памяти (run me) {display-mode: \"form\"}\n",
        "\n",
        "# NamedTuple to store memory\n",
        "EpisodeRewardsToGoMemory = collections.namedtuple(\"EpisodeRewardsToGoMemory\", [\"obs\", \"action\", \"reward_to_go\"])\n",
        "\n",
        "class EpisodeRewardsToGoBuffer:\n",
        "\n",
        "    def __init__(self, num_transitions_to_store=512, batch_size=256):\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "        self.current_episode_transition_buffer = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_transition_buffer.append(transition)\n",
        "\n",
        "        if transition.done:\n",
        "\n",
        "            episode_rewards = []\n",
        "            for t in self.current_episode_transition_buffer:\n",
        "                episode_rewards.append(t.reward)\n",
        "\n",
        "            r2g = compute_rewards_to_go(episode_rewards)\n",
        "\n",
        "            for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "                memory = EpisodeRewardsToGoMemory(t.obs, t.action, r2g[i])\n",
        "                self.memory_buffer.append(memory)\n",
        "\n",
        "            # Reset episode buffer\n",
        "            self.current_episode_transition_buffer = []\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "    def sample(self):\n",
        "        random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "\n",
        "        obs_batch, action_batch, reward_to_go_batch = zip(*random_memory_sample)\n",
        "\n",
        "        return EpisodeRewardsToGoMemory(\n",
        "            np.stack(obs_batch).astype(\"float32\"),\n",
        "            np.asarray(action_batch).astype(\"int32\"),\n",
        "            np.asarray(reward_to_go_batch).astype(\"int32\")\n",
        "        )\n",
        "\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeRewardsToGoBuffer(num_transitions_to_store=512, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idkav_aSYXvz"
      },
      "source": [
        "### Policy neural network\n",
        "Далее мы будем использовать простую нейронную сеть для аппроксимации политики. Наша нейронная сеть политики будет иметь входной слой, который принимает наблюдение в качестве входных данных и пропускает его через два скрытых слоя, а затем выводит одно скалярное значение для каждого из возможных действий. Таким образом, в CartPole выходной слой будет иметь размер `2`.\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) — это библиотека для реализации нейронных сетей в JAX. Ниже мы реализовали простую функцию для создания сети политики для вас.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2XO7VkORjM4"
      },
      "outputs": [],
      "source": [
        "def make_policy_network(num_actions: int, layers=[20, 20]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for the policy.\"\"\"\n",
        "\n",
        "  def policy_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [\n",
        "            hk.Flatten(),\n",
        "            hk.nets.MLP(layers + [num_actions])\n",
        "        ]\n",
        "    )\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(policy_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR2y8FjaG-G"
      },
      "source": [
        "У сетей Haiku есть две важные функции, о которых вам нужно знать. Первая — `network.init(<random_key>, <input>)`, которая возвращает набор случайных начальных параметров. Второй метод — `network.apply(<params>, <input>)`, который передает входные данные через сеть, используя предоставленный набор параметров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJrn9o-Vatkw",
        "outputId": "dd652c93-ee02-4e4e-e8f9-3dd21bd973dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial params: dict_keys(['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2'])\n",
            "Policy network output: [ 0.91155875 -0.3961737 ]\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "POLICY_NETWORK = make_policy_network(num_actions=num_actions, layers=[20,20])\n",
        "random_key = jax.random.PRNGKey(42) # random key\n",
        "dummy_obs = np.ones(obs_shape, \"float32\")\n",
        "\n",
        "# Initialise parameters\n",
        "REINFORCE_params = POLICY_NETWORK.init(random_key, dummy_obs)\n",
        "print(\"Initial params:\", REINFORCE_params.keys())\n",
        "\n",
        "# Pass input through the network\n",
        "output = POLICY_NETWORK.apply(REINFORCE_params, dummy_obs)\n",
        "print(\"Policy network output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlouUBvoeunz"
      },
      "source": [
        "Выходы нашей сети политик — [логиты](https://qr.ae/pv4YTe). Чтобы преобразовать это в распределение вероятностей по действиям, мы передаем логиты в функцию [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "### УСИЛЕНИЕ выбор функции действия\n",
        "\n",
        "**Упражнение 8:** Завершите функцию ниже, которая берет вектор логитов и случайным образом выбирает действие из категориального распределения, заданного логитами.\n",
        "\n",
        "**Полезные функции:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Z8DxUmeOGJ"
      },
      "outputs": [],
      "source": [
        "def sample_action(random_key, logits):\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  action = ...\n",
        "\n",
        "  # END YOUR code\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5opHJMO0D_Ub",
        "outputId": "3fa75928-56bc-4ae8-868b-3e2056d14e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seems correct.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 8 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  random_key = jax.random.PRNGKey(42) # random key\n",
        "  action = sample_action(random_key, np.array([1,2], \"float32\"))\n",
        "  if action != 1:\n",
        "    print(\"Your function is incorrect.\")\n",
        "  else:\n",
        "    print(\"Seems correct.\")\n",
        "except Exception as e:\n",
        "    print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5UH87VRjM4"
      },
      "source": [
        "Теперь мы можем реализовать функцию `REINFORCE_choose_action`. Мы передадим наблюдение через сеть политики для вычисления логитов, а затем передадим логиты в функцию `sample_action` для выбора и действия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJTzrDAZ0Ul5"
      },
      "outputs": [],
      "source": [
        "def REINFORCE_choose_action(key, params, actor_state, obs, evaluation=False):\n",
        "  obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim before passing through network\n",
        "\n",
        "  # Pass obs through policy network to compute logits\n",
        "  logits = POLICY_NETWORK.apply(params, obs)\n",
        "  logits = logits[0] # remove batch dim\n",
        "\n",
        "  # Randomly sample action\n",
        "  sampled_action = sample_action(key, logits)\n",
        "\n",
        "  return sampled_action, actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI26SLAb7iRo"
      },
      "source": [
        "Теперь, когда мы реализовали функцию `REINFORCE_choose_action`, нам осталось только создать функцию `REINFORCE_learn`. Функция обучения должна использовать функцию `weighted_log_prob`, которую мы создали ранее, чтобы вычислить потери градиента политики и применить обновления градиента к нашей нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ALCJESQJ8e"
      },
      "source": [
        "### Policy gradient loss\n",
        "\n",
        "\n",
        "**Упражнение 9:** Завершите функцию `policy_gradient_loss` ниже. Функция должна вычислять вероятности действий, пропуская `logits` через функцию softmax. Затем следует извлечь вероятность заданного `action` (используя индексацию массива) и вычислить `weighted_log_prob`, используя функцию, которую мы создали ранее.\n",
        "\n",
        "**Полезные методы:**\n",
        "*   `jax.nn.softmax` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sUKkqx0RjM4"
      },
      "outputs": [],
      "source": [
        "def policy_gradient_loss(action, logits, reward_to_go):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  all_action_probs = ... # convert logits into probs\n",
        "\n",
        "  action_prob = ... # using array indexing to get prob of action\n",
        "\n",
        "  weighted_log_prob = ...\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  loss = - weighted_log_prob # negative because we want gradient `ascent`\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AMJvau1FsM5",
        "outputId": "e180ba5e-2989-4e09-ad5d-0f0e21efac1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your implementation looks wrong.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 9 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = policy_gradient_loss(1, np.array([1,2], \"float32\"), 10)\n",
        "  if result != 3.1326175:\n",
        "    print(\"Your implementation looks wrong.\")\n",
        "  else:\n",
        "    print(\"Looks correct.\")\n",
        "except Exception as e:\n",
        "  print(\"An Error Occured: {}\".format(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzuqx1jJrwVx"
      },
      "source": [
        "Когда мы делаем шаг обновления градиента политики, мы захотим сделать это, используя пакет опыта, а не просто один опыт, как выше. Мы можем использовать функцию JAX [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap), чтобы легко заставить нашу функцию `policy_gradient_loss` работать с пакетом опыта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yq4naLURjM4",
        "outputId": "1173f65d-e9f3-4cc0-baaf-d6043baf11d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy gradient loss on batch: 1.6967314\n"
          ]
        }
      ],
      "source": [
        "def batched_policy_gradient_loss(params, obs_batch, action_batch, reward_to_go_batch):\n",
        "    # Get logits by passing observation through network\n",
        "    logits_batch = POLICY_NETWORK.apply(params, obs_batch)\n",
        "\n",
        "    policy_gradient_loss_batch = jax.vmap(policy_gradient_loss)(\n",
        "        action_batch,\n",
        "        logits_batch,\n",
        "        reward_to_go_batch) # add batch\n",
        "\n",
        "    # Compute mean loss over batch\n",
        "    mean_policy_gradient_loss = jnp.mean(policy_gradient_loss_batch)\n",
        "\n",
        "    return mean_policy_gradient_loss\n",
        "\n",
        "# TEST\n",
        "obs_batch = np.ones((3, *obs_shape), \"float32\")\n",
        "actions_batch = np.array([1,0,0])\n",
        "rew2go_batch = np.array([2.3, 4.3, 2.1])\n",
        "\n",
        "loss = batched_policy_gradient_loss(REINFORCE_params, obs_batch, actions_batch, rew2go_batch)\n",
        "\n",
        "print(\"Policy gradient loss on batch:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDhTH3culwqo"
      },
      "source": [
        "### Оптимизатор сети\n",
        "\n",
        "Чтобы применить обновления градиента политики к нашей нейронной сети, мы будем использовать библиотеку JAX под названием [Optax](https://github.com/deepmind/optax). Optax имеет реализацию [оптимизатора Adam](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/), которую мы можем использовать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXINlMHP5Ic"
      },
      "outputs": [],
      "source": [
        "REINFORCE_OPTIMIZER = optax.adam(1e-3)\n",
        "\n",
        "# Initialise the optimiser\n",
        "REINFORCE_optim_state = REINFORCE_OPTIMIZER.init(REINFORCE_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViENrHOALbCw"
      },
      "source": [
        "Теперь у нас есть все необходимое для создания функции `REINFORCE_learn`. Мы сохраним состояние оптимизатора в `learn_state`. Мы вычислим градиент потерь градиента политики с помощью  `jax.grad` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQr2Uz5ORjM5"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the state of the optimiser\n",
        "REINFORCELearnState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "\n",
        "def REINFORCE_learn(key, params, learner_state, memory):\n",
        "\n",
        "  # Get the policy gradient by using `jax.grad()` on `batched_policy_gradient_loss`\n",
        "  grad_loss = jax.grad(batched_policy_gradient_loss)(params, memory.obs, memory.action, memory.reward_to_go)\n",
        "\n",
        "  # Get param updates using gradient and optimizer\n",
        "  updates, new_optim_state = REINFORCE_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "  # Apply updates to params\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return params, REINFORCELearnState(new_optim_state) # update learner state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an3U2NhRKgG"
      },
      "source": [
        "### Цикл обучения REINFORCE\n",
        "Теперь мы можем обучить нашего агента REINFORCE, собрав все воедино с помощью цикла обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "vioIcVGsRjM5",
        "outputId": "3eb65e69-b008-44e9-9b49-faf4219fdc0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training. This may take up to 10 minutes to complete.\n",
            "Episode: 0\tEpisode Return: 12.0\tAverage Episode Return: 12.0\tEvaluator Episode Return: 17.875\n",
            "Episode: 100\tEpisode Return: 43.0\tAverage Episode Return: 110.5\tEvaluator Episode Return: 105.25\n",
            "Episode: 200\tEpisode Return: 200.0\tAverage Episode Return: 158.85\tEvaluator Episode Return: 182.5\n",
            "Episode: 300\tEpisode Return: 200.0\tAverage Episode Return: 185.6\tEvaluator Episode Return: 185.875\n",
            "Episode: 400\tEpisode Return: 200.0\tAverage Episode Return: 184.5\tEvaluator Episode Return: 189.25\n",
            "Episode: 500\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n",
            "Episode: 600\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n",
            "Episode: 700\tEpisode Return: 179.0\tAverage Episode Return: 173.7\tEvaluator Episode Return: 183.375\n",
            "Episode: 800\tEpisode Return: 200.0\tAverage Episode Return: 186.1\tEvaluator Episode Return: 200.0\n",
            "Episode: 900\tEpisode Return: 200.0\tAverage Episode Return: 186.15\tEvaluator Episode Return: 137.75\n",
            "Episode: 1000\tEpisode Return: 138.0\tAverage Episode Return: 128.05\tEvaluator Episode Return: 134.125\n",
            "Episode: 1100\tEpisode Return: 200.0\tAverage Episode Return: 181.35\tEvaluator Episode Return: 196.625\n",
            "Episode: 1200\tEpisode Return: 131.0\tAverage Episode Return: 194.3\tEvaluator Episode Return: 184.25\n",
            "Episode: 1300\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n",
            "Episode: 1400\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 194.875\n",
            "Episode: 1500\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n",
            "Episode: 1600\tEpisode Return: 200.0\tAverage Episode Return: 195.65\tEvaluator Episode Return: 189.625\n",
            "Episode: 1700\tEpisode Return: 114.0\tAverage Episode Return: 193.9\tEvaluator Episode Return: 200.0\n",
            "Episode: 1800\tEpisode Return: 200.0\tAverage Episode Return: 199.6\tEvaluator Episode Return: 200.0\n",
            "Episode: 1900\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n",
            "Episode: 2000\tEpisode Return: 200.0\tAverage Episode Return: 200.0\tEvaluator Episode Return: 200.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9e00c59e7e94>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training. This may take up to 10 minutes to complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m episode_returns, evaluator_returns = run_training_loop(\n\u001b[0m\u001b[1;32m     11\u001b[0m                                         \u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                         \u001b[0mREINFORCE_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-df5e1941bf3d>\u001b[0m in \u001b[0;36mrun_training_loop\u001b[0;34m(env_name, agent_params, agent_select_action_func, agent_actor_state, agent_learn_func, agent_learner_state, agent_memory, num_episodes, evaluator_period, evaluation_episodes, learn_steps_per_episode, train_every_timestep, video_subdir)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Step environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Pack into transition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mtruncateds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0minfos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         ) = step_api_compatibility(self.env.step(action), True, self.is_vector_env)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/renderer.py\u001b[0m in \u001b[0;36mrender_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mrender_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_returns_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single_rgb_array\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             return np.transpose(\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixels3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             )\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# JIT the choose_action and learn functions for more speed\n",
        "REINFORCE_learn_jit = jax.jit(REINFORCE_learn)\n",
        "REINFORCE_choose_action_jit = jax.jit(REINFORCE_choose_action)\n",
        "\n",
        "# Initial learn state\n",
        "REINFORCE_learn_state = REINFORCELearnState(REINFORCE_optim_state)\n",
        "\n",
        "# Run training loop\n",
        "print(\"Starting training. This may take up to 10 minutes to complete.\")\n",
        "episode_returns, evaluator_returns = run_training_loop(\n",
        "                                        env_name,\n",
        "                                        REINFORCE_params,\n",
        "                                        REINFORCE_choose_action_jit,\n",
        "                                        None, # action state not used\n",
        "                                        REINFORCE_learn_jit,\n",
        "                                        REINFORCE_learn_state,\n",
        "                                        REINFORCE_memory,\n",
        "                                        num_episodes=1001,\n",
        "                                        learn_steps_per_episode=2,\n",
        "                                        video_subdir=\"reinforce\"\n",
        "                                      )\n",
        "\n",
        "# Plot the episode returns\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.title(\"REINFORCE\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "caKL3ngNr_Yh",
        "outputId": "ef7ad069-ea56-40ab-8ae3-7f674b199a4c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<video width=400 controls>\n",
              "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Визуализация {display-mode: \"form\"}\n",
        "#@markdown Выберите номер эпизода, менее или равного 1000, и **run this cell**.\n",
        "\n",
        "episode_number = 1900 #@param {type:\"number\"}\n",
        "\n",
        "assert (episode_number % 100) == 0, \"Episode number must be a multiple of 100 since we only record every 100th episode.\"\n",
        "assert episode_number < 1001, \"Episode number must be less than or equal to 1000\"\n",
        "\n",
        "eval_episode_number = int(episode_number / 100 * 8)\n",
        "video_path = f\"./video/reinforce/eval/rl-video-episode-{eval_episode_number}.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HHzFTOc-Qr"
      },
      "source": [
        "## 4. Q-Learning\n",
        "Другим распространенным подходом к поиску оптимальной политики в среде с использованием RL является Q-обучение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1nOZrUSzhE"
      },
      "source": [
        "### Функция значения состояния-действия\n",
        "В Q-обучении агент изучает функцию, которая аппроксимирует **значение** пар состояние-действие. Под *значением* мы подразумеваем отдачу, которую вы ожидаете получить, если начнете в определенном состоянии $s_t$, выполните определенное действие $a_t$, а затем будете действовать в соответствии с определенной политикой $\\pi$ навсегда. Функция значения состояния-действия политики $\\pi$ задается как\n",
        "\n",
        "$Q_\\pi(s,a)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_t=a\\right]$.\n",
        "\n",
        "Мы говорим, что функция значения $Q_\\pi(s,a)$ является **оптимальной** функцией значения, если политика $\\pi$ является оптимальной политикой. Обозначим функцию оптимального значения следующим образом:\n",
        "\n",
        "$Q_\\ast(s,a)=\\max \\limits_\\pi \\ \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_0=a\\right]$\n",
        "\n",
        "Существует важная связь между оптимальным действием $a_\\ast$ в состоянии $s$ и оптимальной функцией значения состояния-действия $Q_\\ast$. А именно, оптимальное действие $a_\\ast$ в состоянии $s$ равно действию, которое максимизирует оптимальную функцию значения состояния-действия. Эта связь естественным образом индуцирует оптимальную политику:\n",
        "\n",
        "$\\pi_\\ast(s)=\\arg \\max \\limits_a\\ Q_\\ast(s, a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x2tqvZSihz"
      },
      "source": [
        "### Greedy action selection (Жадный выбор действия)\n",
        "\n",
        "**Упражнение 10:** Давайте реализуем функцию, которая, учитывая вектор Q-значений, возвращает действие с наибольшим Q-значением (т. е. жадное действие).\n",
        "\n",
        "**Useful methods:**\n",
        "*   `jax.numpy.argmax` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9P1YdzTIDU"
      },
      "outputs": [],
      "source": [
        "# Implement a function takes q-values as input and returns the greedy_action\n",
        "def select_greedy_action(q_values):\n",
        "\n",
        "  # YOUR CODE\n",
        "  action = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBzLr_G7QKXR",
        "outputId": "cbc2bac8-3c99-45f9-c820-c703b7e92bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks good.\n"
          ]
        }
      ],
      "source": [
        "# @title Проверка упр 10 (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  q_values = jnp.array([1,1,3,4])\n",
        "  action = select_greedy_action(q_values)\n",
        "\n",
        "  if action != 3:\n",
        "    print(\"Incorrect answer, your greedy action selector looks wrong\")\n",
        "  else:\n",
        "    print(\"Looks good.\")\n",
        "except Exception as e:\n",
        "  print(\"An Error Occured: {}\".format(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQUlqZ4ZyLp"
      },
      "source": [
        "### Q-Network\n",
        "В отличие от подхода с градиентом политики из предыдущего раздела, в Q-обучении и других методах RL на основе значений нам не нужна параметризация для политики, вместо этого мы параметризуем Q-функцию с помощью нейронной сети $Q_\\theta$. Мы получаем политику из Q-сети, всегда выбирая действие с *наибольшим* значением:\n",
        "\n",
        "$\\hat{\\pi}_\\theta(s)=\\arg \\max \\limits_a\\ Q_{\\theta}(s, a)$\n",
        "\n",
        "Как и ранее, мы будем использовать хайку для создания нейронной сети для аппроксимации этой Q-функции. Сеть будет принимать наблюдение в качестве входных данных, а затем выводить Q-значение для каждого из доступных действий. Таким образом, в случае CartPole выход сети будет иметь размер $2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wU1soJYZyLp"
      },
      "outputs": [],
      "source": [
        "def build_network(num_actions: int, layers=[20, 20]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
        "\n",
        "  def q_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Flatten(),\n",
        "         hk.nets.MLP(layers + [num_actions])])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(q_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwG-4qTS1zx"
      },
      "source": [
        "Давайте инициализируем нашу Q-сеть и получим начальные параметры."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uvq5n2cS9lu",
        "outputId": "4fafa32f-2041-4593-adef-09a4efbebcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-Learning params: dict_keys(['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2'])\n"
          ]
        }
      ],
      "source": [
        "# Initialise Q-network\n",
        "Q_NETWORK = build_network(num_actions=num_actions, layers=[20, 20]) # two actions\n",
        "\n",
        "dummy_obs = jnp.zeros((1,*obs_shape), jnp.float32) # a dummy observation like the one in CartPole\n",
        "\n",
        "random_key = jax.random.PRNGKey(42) # random key\n",
        "Q_NETWORK_PARAMS = Q_NETWORK.init(random_key, dummy_obs) # Get initial params\n",
        "\n",
        "print(\"Q-Learning params:\", Q_NETWORK_PARAMS.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqCUeZfhEfyP"
      },
      "source": [
        "Прежде чем мы реализуем функцию потерь, необходимую для обучения нашей Q-сети, давайте сначала обсудим интуицию, стоящую за ней."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbLig3uSZyLp"
      },
      "source": [
        "### The Bellman Equations (Уравнения Беллмана)\n",
        "Функция значения может быть записана рекурсивно как:\n",
        "\n",
        "$Q_{\\pi}(s, a) =\\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+ \\underset{a^{\\prime} \\sim \\pi}{\\mathrm{E}}\\left[Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\right]$,\n",
        "\n",
        "где $s' \\sim P$ — это сокращение для обозначения того, что следующее состояние $s'$ выбирается из функции перехода среды $P(s'\\mid s,a)$. Интуитивно это уравнение говорит, что ценность действия $a$, которое вы предприняли в состоянии $s$, равна вознаграждению $r$, которое вы ожидаете получить, плюс ценность, которую вы ожидаете получить в следующем состоянии $s`$, в котором вы окажетесь, при условии, что вы выберете свое следующее действие $a`$ с политикой $\\pi$. Уравнение Беллмана для оптимальной функции ценности выглядит так:\n",
        "\n",
        "$Q_{*}(s, a) =\\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\ \\underset{a^{\\prime}}{\\max}\\ Q_{*}(s^{\\prime}, a^{\\prime})\\right]$\n",
        "\n",
        "Обратите внимание, что вместо выбора вашего следующего действия $a`$ с политикой $\\pi$ мы выбираем действие с наибольшим значением Q.\n",
        "\n",
        "Более подробное обсуждение уравнений Беллмана можно найти на сайте [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOJi5G8ZyLp"
      },
      "source": [
        "### The Bellman Backup (Резервное копирование Беллмана)\n",
        "\n",
        "Чтобы научиться аппроксимировать оптимальную функцию Q-значения, мы можем использовать правую часть уравнения Беллмана в качестве правила обновления. Другими словами, предположим, что у нас есть Q-сеть $Q_\\theta$, аппроксимированная с использованием параметров $\\theta$, тогда мы можем итеративно обновлять параметры таким образом, что\n",
        "\n",
        "$Q_\\theta(s,a)\\leftarrow r(s, a) + \\underset{a'}{\\max}\\ Q_\\theta(s', a')$.\n",
        "\n",
        "Интуитивно это означает, что аппроксимация Q-значения действия $a$ в состоянии $s$ должна быть обновлена ​​таким образом, чтобы она была ближе к равной вознаграждению, полученному от среды $r(s, a)$, плюс значение наилучшего возможного действия в следующем состоянии $s'$. Мы можем выполнить эту оптимизацию, минимизируя разницу между левой и правой частью относительно параметров $\\theta$, используя градиентный спуск. Мы можем измерить разницу между двумя значениями, используя [квадратичную ошибку](https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function).\n",
        "\n",
        "**Упражнение 11:** Реализуйте функцию квадратичной ошибки.\n",
        "\n",
        "**Полезные функции**\n",
        "* `jax.numpy.square` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.square.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTto__ohZyLp"
      },
      "outputs": [],
      "source": [
        "def compute_squared_error(pred, target):\n",
        "  # YOUR CODE\n",
        "  squared_error = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGhX8XTFVPVU",
        "outputId": "6385fc99-a7bd-4a3f-9544-ccf7c21989c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An Error Occured: name 'compute_squared_error' is not defined\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 11 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = compute_squared_error(1, 4)\n",
        "\n",
        "  if result != 9:\n",
        "    print(\"Your implementation looks wrong.\")\n",
        "  else:\n",
        "    print(\"Looks good.\")\n",
        "except Exception as e:\n",
        "  print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycpZVkgdZyLp"
      },
      "source": [
        "**Упражнение 12:** Реализуйте функцию, которая вычисляет **цель Беллмана** (правая часть уравнения Беллмана). Если эпизод находится на последнем временном шаге (т. е. done==1.0), то цель Беллмана должна быть равна награде, без дополнительного значения в конце.\n",
        "\n",
        "**Полезные функции**\n",
        "* `jax.numpy.max` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.max.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "961_OllWZyLp"
      },
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values):\n",
        "  \"\"\"A function to compute the bellman target.\n",
        "\n",
        "  Args:\n",
        "      reward: a scalar reward.\n",
        "      done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "      next_q_values: a vector of q_values for the next state. One for each action.\n",
        "  Returns:\n",
        "      A scalar equal to the bellman target.\n",
        "\n",
        "  \"\"\"\n",
        "  # YOUR CODE\n",
        "  bellman_target = ...\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return bellman_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5RAhegOWAkC",
        "outputId": "a3ee7359-ea0e-49fc-9f67-51ac06f5fe02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks good.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 12 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  # not done\n",
        "  result1 = compute_bellman_target(1, 0.0, np.array([3,2], \"float32\"))\n",
        "\n",
        "  # done\n",
        "  result2 = compute_bellman_target(1, 1.0, np.array([3,2], \"float32\"))\n",
        "\n",
        "  if result1 != 4 or result2 != 1:\n",
        "    print(\"Your implementation looks wrong.\")\n",
        "  else:\n",
        "    print(\"Looks good.\")\n",
        "except Exception as e:\n",
        "  print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIjrHSJZyLq"
      },
      "source": [
        "Теперь мы можем объединить эти две функции, чтобы вычислить потерю для Q-обучения. Потери Q-обучения равны квадрату разницы между прогнозируемым значением Q действия и его соответствующей целью Беллмана.\n",
        "\n",
        "**Упражнение 13:** Реализуйте функцию потери Q-обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJY_kpFcZyLq"
      },
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "\n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "    chosen_action_q_value = ... # q_value of action, use array indexing\n",
        "    bellman_target = ...\n",
        "    squared_error = ...\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aZRn1qaWx2M",
        "outputId": "0089f002-9c92-484e-97e5-2071ce76e0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks good.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 13 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = q_learning_loss(np.array([3,2], \"float32\"), 1, 2, 0.0, np.array([3,2], \"float32\"))\n",
        "\n",
        "  if result != 9.0:\n",
        "    print(\"Your implementation looks wrong.\")\n",
        "  else:\n",
        "    print(\"Looks good.\")\n",
        "except Exception as e:\n",
        "  print(\"An Error Occured: {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4YnSUfJZyLq"
      },
      "source": [
        "### Target Q-network (Целевая Q-сеть)\n",
        "Обратите внимание: когда мы вычисляем цель Беллмана, мы используем нашу Q-сеть $Q_\\theta$ для вычисления значения для следующего состояния $s_t$. По сути, мы используем наше последнее приближение Q-функции для вычисления цели нашего следующего приближения. Использование приближения для вычисления цели для вашего следующего приближения называется самозагрузкой. К сожалению, если мы наивно делаем такую ​​самозагрузку, это может сделать обучение нейронной сети очень нестабильным. Чтобы смягчить это, мы можем вместо этого использовать другой набор параметров $\\hat{\\theta}$ для вычисления значений в состоянии $s_{t+1}$. Мы будем сохранять параметры $\\hat{\\theta}$ фиксированными и только периодически обновлять их, чтобы они были равны последним онлайн-параметрам $\\theta$ каждые пару шагов обучения *(скажем, 100)*. Это позволяет сохранять фиксированные цели Беллмана для пары шагов обучения, чтобы помочь снизить нестабильность из-за самозагрузки.\n",
        "\n",
        "Нам нужно будет отслеживать последние (онлайн) параметры, а также параметры целевых сетей. Давайте создадим `NamedTuple` для хранения этих двух значений. Нам также нужно будет отслеживать количество шагов обучения, которые мы сделали, чтобы знать, когда обновлять целевую сеть. Давайте сохраним `count` шагов обучения в `learn_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvZqUKmq6L7k"
      },
      "outputs": [],
      "source": [
        "# Store online and target parameters\n",
        "QLearnParams = collections.namedtuple(\"Params\", [\"online\", \"target\"])\n",
        "\n",
        "# Q-learn-state\n",
        "QLearnState = collections.namedtuple(\"LearnerState\", [\"count\", \"optim_state\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJWH2_kNZsau"
      },
      "source": [
        "Мы снова будем использовать Optax для оптимизации нашей нейронной сети в JAX. Мы сохраняем состояние оптимизатора в `learn_state` выше. Давайте теперь создадим экземпляр оптимизатора и добавим начальные параметры Q-сети в объект `QLearnParams`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwqKTN6BaAXE"
      },
      "outputs": [],
      "source": [
        "# Initialise Q-network optimizer\n",
        "Q_LEARN_OPTIMIZER = optax.adam(3e-4) # learning rate\n",
        "\n",
        "Q_LEARN_OPTIM_STATE = Q_LEARN_OPTIMIZER.init(Q_NETWORK_PARAMS) # initial optim state\n",
        "\n",
        "# Create Learn State\n",
        "Q_LEARNING_LEARN_STATE = QLearnState(0, Q_LEARN_OPTIM_STATE) # count set to zero initially\n",
        "\n",
        "# Add initial Q-network weights to QLearnParams object\n",
        "Q_LEARNING_PARAMS = QLearnParams(online=Q_NETWORK_PARAMS, target=Q_NETWORK_PARAMS) # target equal to online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj89M8LgZlFe"
      },
      "source": [
        "Теперь мы можем реализовать простую функцию, которая обновляет параметры целевых сетей, чтобы они соответствовали последним онлайн-параметрам каждые 100 шагов обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKrg_3rO6cCL"
      },
      "outputs": [],
      "source": [
        "def update_target_params(learn_state, online_weights, target_weights):\n",
        "  \"\"\"A function to update target params every 100 training steps\"\"\"\n",
        "\n",
        "  target = jax.lax.cond(\n",
        "      jax.numpy.mod(learn_state.count, 100) == 0,\n",
        "      lambda x, y: x,\n",
        "      lambda x, y: y,\n",
        "      online_weights,\n",
        "      target_weights\n",
        "  )\n",
        "\n",
        "  params = QLearnParams(online_weights, target)\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoiaYSo9ZyLq"
      },
      "source": [
        "### Q-learning loss (Функция потери Q-обучения)\n",
        "Теперь у нас есть все необходимое для реализации функции `q_learn`, которая принимает некоторую партию переходов и выполняет шаг Q-обучения для обновления параметров сети. Но сначала мы используем `jax.vmap` для изменения функции `q_learning_loss` так, чтобы она принимала партии переходов. Кроме того, мы вычислим Q-значения, передавая наблюдения через `Q_NETWORK`, и целевые Q-значения, используя целевые параметры `Q_NETWORK`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnrsGppWZyLq"
      },
      "outputs": [],
      "source": [
        "def batched_q_learning_loss(online_params, target_params, obs, actions, rewards, next_obs, dones):\n",
        "    q_values = Q_NETWORK.apply(online_params, obs) # use the online parameters\n",
        "    next_q_values = Q_NETWORK.apply(target_params, next_obs) # use the target parameters\n",
        "    squared_error = jax.vmap(q_learning_loss)(q_values, actions, rewards, dones, next_q_values) # vmap q_learning_loss\n",
        "    mean_squared_error = jnp.mean(squared_error) # mean squared error over batch\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU8vo9ZebnEa"
      },
      "source": [
        "Теперь мы можем создать функцию `q_learn`, которая вычисляет градиент `batched_q_learning_loss`, а затем использует оптимизатор Optax для обновления весов сети, а затем, наконец (возможно), обновляет целевые параметры."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BYoX2W_ZyLr"
      },
      "outputs": [],
      "source": [
        "def q_learn(rng, params, learner_state, memory):\n",
        "  # Compute gradients\n",
        "  grad_loss = jax.grad(batched_q_learning_loss)(params.online, params.target, memory.obs,\n",
        "                                          memory.action, memory.reward,\n",
        "                                          memory.next_obs, memory.done,\n",
        "                                          ) # jax.grad\n",
        "\n",
        "  # Get updates\n",
        "  updates, opt_state = Q_LEARN_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "  # Apply them\n",
        "  new_weights = optax.apply_updates(params.online, updates)\n",
        "\n",
        "  # Maybe update target network\n",
        "  params = update_target_params(learner_state, new_weights, params.target)\n",
        "\n",
        "  # Increment learner step counter\n",
        "  learner_state = QLearnState(learner_state.count + 1, opt_state)\n",
        "\n",
        "  return params, learner_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZsKHssZyLq"
      },
      "source": [
        "### Replay Buffer (Буфер воспроизведения)\n",
        "Для Q-обучения нам понадобится память агента, которая хранит целые переходы: `obs`, `action`, `reward`, `next_obs`, `done`. Когда мы извлекаем переходы из памяти, они должны выбираться случайным образом из всех переходов, собранных до сих пор. В RL мы часто называем такой модуль **буфером воспроизведения**.\n",
        "\n",
        "Одним из преимуществ использования такого буфера воспроизведения является то, что опыт можно повторно использовать несколько раз для обучения, в отличие от алгоритма градиента политики REINFORCE, где мы отбрасывали воспоминания после их использования для обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tv5dUH6ZyLr"
      },
      "outputs": [],
      "source": [
        "class TransitionMemory(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, max_size=10_000, batch_size=256):\n",
        "    self.batch_size = batch_size\n",
        "    self.buffer = collections.deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, transition):\n",
        "\n",
        "    # add transition to the replay buffer\n",
        "    self.buffer.append(\n",
        "        (transition.obs, transition.action, transition.reward,\n",
        "          transition.next_obs, transition.done)\n",
        "    )\n",
        "\n",
        "\n",
        "  def is_ready(self):\n",
        "    return self.batch_size <= len(self.buffer)\n",
        "\n",
        "  def sample(self):\n",
        "    # Randomly sample a batch of transitions from the buffer\n",
        "    random_replay_sample = random.sample(self.buffer, self.batch_size)\n",
        "\n",
        "    # Batch the transitions together\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*random_replay_sample)\n",
        "\n",
        "    return Transition(\n",
        "        np.stack(obs_batch).astype(\"float32\"),\n",
        "        np.asarray(action_batch).astype(\"int32\"),\n",
        "        np.asarray(reward_batch).astype(\"float32\"),\n",
        "        np.stack(next_obs_batch).astype(\"float32\"),\n",
        "        np.asarray(done_batch).astype(\"float32\")\n",
        "    )\n",
        "\n",
        "# Instantiate the memory\n",
        "Q_LEARNING_MEMORY = TransitionMemory(max_size=50_000, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHk03VVUHAV"
      },
      "source": [
        "### Случайное исследование\n",
        "У нас почти есть все, что нужно для функционирующего агента Q-обучения. Но одна проблема заключается в том, что если мы всегда выбираем действие с наивысшим значением Q, то политика агента будет полностью [детерминированной](https://www.quora.com/What-is-the-intuitive-difference-between-a-stochastic-model-and-a-deterministic-model). Это означает, что агент всегда будет выбирать одну и ту же стратегию. Это может представлять проблему, поскольку в начале обучения Q-сеть будет очень неточной (т. е. плохой аппроксимацией истинной Q-функции). Таким образом, агент будет последовательно выбирать неоптимальные действия. Более того, агент никогда не отклонится от своей неоптимальной стратегии и никогда не обнаружит новые, потенциально более полезные действия. В результате Q-сеть остается неточной. В идеале агент должен опробовать много разных стратегий, чтобы он мог наблюдать результаты (вознаграждения) своих действий в разных состояниях и таким образом улучшить приближение Q-функции.\n",
        "\n",
        "Один простой способ гарантировать, что агент опробует много разных действий, — позволить ему периодически выбирать некоторые случайные действия вместо жадного (лучшего) действия все время.\n",
        "\n",
        "**Упражнение 14:** Реализуйте функцию, которая, учитывая количество возможных (дискретных) действий, возвращает случайное действие.\n",
        "\n",
        "**Полезные методы:**\n",
        "\n",
        "*  `jax.random.randint` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.randint.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUKkpMLXUtko"
      },
      "outputs": [],
      "source": [
        "def select_random_action(key, num_actions):\n",
        "\n",
        "    # YOUR CODE\n",
        "    action = ...\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO6va6S2Y40E",
        "outputId": "e7f6300f-633a-4a6d-ee27-6fc8cc860c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks good.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 14 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  random_key1 = random_key = jax.random.PRNGKey(6) # random key\n",
        "  random_key2 = random_key = jax.random.PRNGKey(1000) # random key\n",
        "  result1 = select_random_action(random_key1, 2)\n",
        "  result2 = select_random_action(random_key2, 2)\n",
        "\n",
        "  if result1 != 1 or result2 != 0:\n",
        "    print(\"Your implementation looks wrong.\")\n",
        "  else:\n",
        "    print(\"Looks good.\")\n",
        "except:\n",
        "  print(\"Your implementation looks wrong.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kKDFT6XU6y"
      },
      "source": [
        "### $\\varepsilon$-greedy action selection (эпсилон жадный выбор действия)\n",
        "В начале обучения, когда точность Q-сети низкая, агенту стоит в основном совершать случайные действия, чтобы он мог узнать, насколько хороши/плохи действия. Однако по мере повышения точности Q-сети агент должен начать совершать меньше случайных действий и вместо этого начать выбирать жадные действия относительно Q-значений. Выбор лучших действий с учетом текущей Q-сети называется **эксплуатацией**. В RL мы часто называем отношение случайных и жадных действий **эпсилоном** $\\varepsilon$. Эпсилон обычно представляет собой десятичное значение в интервале $[0,1]$, где, например, $\\varepsilon=0.4$ означает, что агент выбирает случайное действие в 40% случаев, а жадное действие — в 60% случаев. В RL принято линейно уменьшать значение эпсилон с течением времени, так что агент становится все более жадным по мере повышения точности его Q-сети в процессе обучения.\n",
        "\n",
        "**Упражнение 15:** Реализуйте функцию, которая принимает количество временных шагов в качестве входных данных и возвращает текущее значение эпсилон."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qejnbCocurG"
      },
      "outputs": [],
      "source": [
        "EPSILON_DECAY_TIMESTEPS = 3000 # decay epsilon over 3000 timesteps\n",
        "EPSILON_MIN = 0.1 # 10% exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ujSbssCZyLs"
      },
      "outputs": [],
      "source": [
        "def get_epsilon(num_timesteps):\n",
        "  # YOUR CODE\n",
        "  epsilon = ... # decay epsilon\n",
        "\n",
        "  epsilon = jax.lax.select(\n",
        "      epsilon < EPSILON_MIN,\n",
        "      ..., # if less than min then set to min\n",
        "      ... # else don't change epsilon\n",
        "  )\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvu4zA64aUou",
        "outputId": "aaaf2f6d-a890-43fa-9cd5-61da89fb8bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your function looks correct.\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упр 15 {display-mode: \"form\"}\n",
        "def check_get_epsilon(get_epsilon):\n",
        "  try:\n",
        "    result1 = get_epsilon(10)\n",
        "    result2 = get_epsilon(5_010)\n",
        "\n",
        "    if result1 != 0.99666667 or result2 != 0.1:\n",
        "      print(\"Your function looks wrong.\")\n",
        "    else:\n",
        "      print(\"Your function looks correct.\")\n",
        "  except:\n",
        "    print(\"Your function looks wrong.\")\n",
        "\n",
        "check_get_epsilon(get_epsilon)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56oo58TVQ_s"
      },
      "source": [
        "**Упражнение 16:** Теперь давайте объединим эти функции для выполнения эпсилон-жадного выбора действия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQx8K4vKUXj"
      },
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "    num_actions = len(q_values) # number of available actions\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    epsilon = ... # get epsilon value\n",
        "\n",
        "    should_explore = ... # hint: a boolean expression to check if some random number is less than epsilon\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        should_explore,\n",
        "        ..., # if should explore\n",
        "        ... # if should be greedy\n",
        "    )\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkVU2a5e7P6b",
        "outputId": "1fabcbba-0f17-4bc5-9726-303dec65ff9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks correct!\n"
          ]
        }
      ],
      "source": [
        "#@title Проверка упражнения 16 {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
        "  dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "  num_timesteps = 5010 # very greedy\n",
        "  actions1 = []\n",
        "  for i in range(10):\n",
        "      actions1.append(int(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps)))\n",
        "\n",
        "  num_timesteps = 0 # completly random\n",
        "  actions2 = []\n",
        "  for i in range(10):\n",
        "      actions2.append(int(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps)))\n",
        "\n",
        "  if actions1 != [1, 1, 0, 1, 1, 0, 1, 1, 1, 1] or actions2 != [0, 0, 0, 1, 1, 1, 1, 0, 0, 0]:\n",
        "    print(\"Looks like something might be incorrect!\")\n",
        "  else:\n",
        "    print(\"Looks correct!\")\n",
        "except:\n",
        "  print(\"Looks like something might be incorrect!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W23MnobN9x"
      },
      "source": [
        "### Действие выбора Q-learning\n",
        "\n",
        "Теперь у нас есть все необходимое для создания функции `q_learning_select_action`. Мы будем использовать `actor_state` для хранения счетчика, который отслеживает текущее количество временных шагов. Мы можем использовать счетчик для уменьшения нашего значения `epsilon`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81TysLc0RjM6"
      },
      "outputs": [],
      "source": [
        "# Actor state stores the current number of timesteps\n",
        "QActorState = collections.namedtuple(\"ActorState\", [\"count\"])\n",
        "\n",
        "def q_learning_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    q_values = Q_NETWORK.apply(params.online, obs)[0] # remove batch dim\n",
        "\n",
        "    action = select_epsilon_greedy_action(key, q_values, actor_state.count)\n",
        "    greedy_action = select_greedy_action(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        greedy_action,\n",
        "        action\n",
        "    )\n",
        "\n",
        "    next_actor_state = QActorState(actor_state.count + 1) # increment timestep counter\n",
        "\n",
        "    return action, next_actor_state\n",
        "\n",
        "Q_LEARNING_ACTOR_STATE = QActorState(0) # counter set to zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884-1oNRGEr"
      },
      "source": [
        "### Обучение\n",
        "Теперь мы можем собрать все вместе, используя цикл агент-среда. Но сначала давайте объединим функцию выбора действия и функцию обучения для дополнительной скорости."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "xbdHDbd1RjM8",
        "outputId": "066f95c6-a2e2-429b-fa37-0e31b23ac1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training. This may take up to 8 minutes to complete.\n",
            "Episode: 0\tEpisode Return: 16.0\tAverage Episode Return: 16.0\tEvaluator Episode Return: 9.75\n",
            "Episode: 100\tEpisode Return: 11.0\tAverage Episode Return: 12.9\tEvaluator Episode Return: 9.375\n",
            "Episode: 200\tEpisode Return: 10.0\tAverage Episode Return: 9.85\tEvaluator Episode Return: 9.5\n",
            "Episode: 300\tEpisode Return: 10.0\tAverage Episode Return: 9.85\tEvaluator Episode Return: 9.375\n",
            "Episode: 400\tEpisode Return: 9.0\tAverage Episode Return: 9.85\tEvaluator Episode Return: 9.5\n",
            "Episode: 500\tEpisode Return: 41.0\tAverage Episode Return: 38.85\tEvaluator Episode Return: 55.875\n",
            "Episode: 600\tEpisode Return: 101.0\tAverage Episode Return: 97.55\tEvaluator Episode Return: 146.625\n",
            "Episode: 700\tEpisode Return: 153.0\tAverage Episode Return: 105.15\tEvaluator Episode Return: 142.25\n",
            "Episode: 800\tEpisode Return: 75.0\tAverage Episode Return: 72.0\tEvaluator Episode Return: 90.5\n",
            "Episode: 900\tEpisode Return: 48.0\tAverage Episode Return: 97.6\tEvaluator Episode Return: 82.875\n",
            "Episode: 1000\tEpisode Return: 63.0\tAverage Episode Return: 76.25\tEvaluator Episode Return: 101.625\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Jit functions\n",
        "q_learning_select_action_jit = jax.jit(q_learning_select_action)\n",
        "q_learn_jit = jax.jit(q_learn)\n",
        "\n",
        "# Run environment loop\n",
        "print(\"Starting training. This may take up to 8 minutes to complete.\")\n",
        "episode_returns, evaluator_returns = run_training_loop(\n",
        "                                        env_name,\n",
        "                                        Q_LEARNING_PARAMS,\n",
        "                                        q_learning_select_action_jit,\n",
        "                                        Q_LEARNING_ACTOR_STATE,\n",
        "                                        q_learn_jit,\n",
        "                                        Q_LEARNING_LEARN_STATE,\n",
        "                                        Q_LEARNING_MEMORY,\n",
        "                                        num_episodes=1001,\n",
        "                                        train_every_timestep=True, # do learning after every timestep\n",
        "                                        video_subdir=\"q_learning\"\n",
        "                                    )\n",
        "\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.title(\"Deep Q-Learning\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k0-41wbpFDE"
      },
      "source": [
        "На этом этапе, как мы надеемся, приближенная Q-функция сошлась к приемлемой политике балансировки шеста в задаче CartPole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "t4v0ZbB4FQUs",
        "outputId": "59e5085a-ee4f-491e-c869-d60d8e8a35ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<video width=400 controls>\n",
              "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Визуализация политики\n",
        "#@markdown Выберите номер эпизода, кратный 100 и меньший или равный 1000, и **запустите эту ячейку**.\n",
        "episode_number = 600 #@param {type:\"number\"}\n",
        "\n",
        "assert (episode_number % 100) == 0, \"Episode number must be a multiple of 100 since we only record every 100th episode.\"\n",
        "assert episode_number < 1001, \"Episode number must be less than or equal to 1000\"\n",
        "\n",
        "\n",
        "eval_episode_number = int(episode_number / 100 * 8)\n",
        "video_path = f\"./video/q_learning/eval/rl-video-episode-{eval_episode_number}.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "Кроме того, существует множество алгоритмов RL, которые вносят существенные улучшения в REINFORCE и Deep Q-Learning. См. эти ресурсы:\n",
        "* [REINFORCE с базовым уровнем](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#baselines-in-policy-gradients)\n",
        "* [Double Deep Q-Network](https://arxiv.org/pdf/1509.06461.pdf)\n",
        "* [Proximal Policy Optimisation (PPO)](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "Если вы ищете более углубленный онлайн-курс по RL, вы можете ознакомиться с этими курсами:\n",
        "* [Reinforcement Learning Foundations on LinkedIn Learning](https://www.linkedin.com/learning/reinforcement-learning-foundations) (создано одним из наших собственных преподавателей, Хаулатом Абдулхакимом)\n",
        "* [Введение в Reinforcement Learning on FreeCodeCamp](https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/)\n",
        "* [Специализация обучения с подкреплением на Coursera](https://www.coursera.org/specializations/reinforcement-learning)\n",
        "\n",
        "Наконец, самый влиятельный учебник по обучению с подкреплением доступен бесплатно онлайн:\n",
        "* [Обучение с подкреплением: введение](http://incompleteideas.net/book/the-book-2nd.html) Ричарда С. Саттона и Эндрю Г. Барто\n",
        "\n",
        "\n",
        "**Ссылки:**\n",
        "\n",
        "* [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)\n",
        "* [Deep Q-Network]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V47Vd_i3qk7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
